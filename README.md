# LLM-Red-Teaming-Adversarial-Evaluation-Suite
This repository contains a comprehensive corpus of Large Language Model (LLM) attack vectors and a Python test harness to evaluate model robustness. It covers techniques ranging from basic prompt injection to advanced cryptographic and mathematical exploits (GCG).
